<!DOCTYPE html><!--Author: Pranav Rajpurkar 2016--><html><head><meta charset="utf-8"><title>The Stanford Question Answering Dataset</title><meta name="description" content="Stanford Question Answering Dataset (SQuAD) is a new reading comprehension dataset, consisting of questions posed by crowdworkers on a set of Wikipedia articles, where the answer to every question is a segment of text, or span, from the corresponding reading passage. With 100,000+ question-answer pairs on 500+ articles, SQuAD is significantly larger than previous reading comprehension datasets."><meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, user-scalable=no"><meta property="og:image" content="/logo.png"><link rel="image_src" type="image/png" href="/SQuAD-explorer/logo.png"><link rel="shortcut icon" href="/SQuAD-explorer/favicon.ico" type="image/x-icon"><link rel="icon" href="/SQuAD-explorer/favicon.ico" type="image/x-icon"><link rel="stylesheet" href="/SQuAD-explorer/bower_components/bootstrap/dist/css/bootstrap.min.css"><link rel="stylesheet" href="/SQuAD-explorer/stylesheets/layout.css"><link rel="stylesheet" href="/SQuAD-explorer/stylesheets/index.css"><script async defer src="https://buttons.github.io/buttons.js"></script><script src="/SQuAD-explorer/javascripts/analytics.js"></script></head><body><div class="navbar navbar-default navbar-fixed-top" id="topNavbar" role="navigation"><div class="container clearfix" id="navContainer"><div class="rightNav"><div class="collapseDiv"><button class="navbar-toggle collapsed" type="button" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar"><span class="glyphicon glyphicon-menu-hamburger"></span></button></div><div class="collapse navbar-collapse" id="navbar"><ul class="nav navbar-nav navbar-right"><li><a href="/SQuAD-explorer/">Home</a></li><li><a href="/SQuAD-explorer/explore/v2.0/dev/">Explore 2.0</a></li><li><a href="/SQuAD-explorer/explore/1.1/dev/">Explore 1.1</a></li></ul></div></div><div class="leftNav"><div class="brandDiv"><a class="navbar-brand" href="/SQuAD-explorer/">SQuAD</a></div></div></div></div><div class="cover" id="topCover"><div class="container"><div class="row"><div class="col-md-12"><h1 id="appTitle">SQuAD<b>2.0</b></h1><h2 id="appSubtitle">The Stanford Question Answering Dataset</h2></div></div></div></div><div class="cover" id="contentCover"><div class="container"><div class="row"><div class="col-md-5"><div class="infoCard"><div class="infoBody"><div class="infoHeadline"><h2>What is SQuAD?</h2></div><p> <span><b>S</b>tanford <b>Qu</b>estion <b>A</b>nswering <b>D</b>ataset (SQuAD) </span>is a reading comprehension dataset, consisting of questions posed by crowdworkers on a set of Wikipedia articles, where the answer to every question is a segment of text, or <i>span</i>, from the corresponding reading passage, or the question might be unanswerable.</p><hr><p><span class="label label-default">New</span><b> SQuAD2.0</b> combines the 100,000 questions in SQuAD1.1 with over 50,000 new, unanswerable questions written adversarially by crowdworkers to look similar to answerable ones. To do well on SQuAD2.0, systems must not only answer questions when possible, but also determine when no answer is supported by the paragraph and abstain from answering.
SQuAD2.0 is a challenging natural language understanding task for existing models, and we release SQuAD2.0 to the community as the successor to SQuAD1.1. 
We are optimistic that this new dataset will encourage the development of reading comprehension systems that know what they don't know.</p><a class="btn actionBtn" href="/SQuAD-explorer/explore/v2.0/dev/">Explore SQuAD2.0 and model predictions</a><a class="btn actionBtn" href="http://arxiv.org/abs/1806.03822">SQuAD2.0 paper (Rajpurkar & Jia et al. '18)</a><hr><p><b> SQuAD 1.1,</b> the previous version of the SQuAD dataset, contains 100,000+ question-answer pairs on 500+ articles.</p><a class="btn actionBtn" href="/SQuAD-explorer/explore/1.1/dev/">Explore SQuAD1.1 and model predictions</a><a class="btn actionBtn" href="http://arxiv.org/abs/1606.05250">SQuAD1.0 paper (Rajpurkar et al. '16)</a><div class="infoHeadline"><h2>Getting Started</h2></div><p>We've built a few resources to help you get started with the dataset.</p><p> Download a copy of the dataset (distributed under the  <a href="http://creativecommons.org/licenses/by-sa/4.0/legalcode">CC BY-SA 4.0</a> license):<ul class="list-unstyled"><li><a class="btn actionBtn inverseBtn" href="/SQuAD-explorer/dataset/train-v2.0.json" download>Training Set v2.0 (40 MB)</a></li><li><a class="btn actionBtn inverseBtn" href="/SQuAD-explorer/dataset/dev-v2.0.json" download>Dev Set v2.0 (4 MB)</a></li></ul></p><p>To evaluate your models, we have also made available the evaluation script we will use for official evaluation, along with a sample prediction file that the script will take as input. To run the evaluation, use <code>python evaluate-v2.0.py &lt;path_to_dev-v2.0&gt; &lt;path_to_predictions&gt;</code>.<ul class="list-unstyled"><li><a class="btn actionBtn inverseBtn" href="https://worksheets.codalab.org/rest/bundles/0x6b567e1cf2e041ec80d7098f031c5c9e/contents/blob/" download>Evaluation Script v2.0</a></li><li><a class="btn actionBtn inverseBtn" href="https://worksheets.codalab.org/bundles/0x8731effab84f41b7b874a070e40f61e2/" download>Sample Prediction File (on Dev v2.0)</a></li></ul></p><p>Once you have a built a model that works to your expectations on the dev set, you submit it to get official scores on the dev and a hidden test set. To preserve the integrity of test results, we do not release the test set to the public. Instead, we require you to submit your model so that we can run it on the test set for you. Here's a tutorial walking you through official evaluation of your model:</p><a class="btn actionBtn inverseBtn" href="https://worksheets.codalab.org/worksheets/0x8212d84ca41c4150b555a075b19ccc05/">Submission Tutorial</a><p>Because SQuAD is an ongoing effort, we expect the dataset to evolve.</p><p>To keep up to date with major changes to the dataset, please subscribe:</p><div id="mc_embed_signup"><form class="validate" id="mc-embedded-subscribe-form" action="//google.us13.list-manage.com/subscribe/post?u=1842e6560d6e10316b4e1aaf5&amp;id=76586bdcf4" method="post" name="mc-embedded-subscribe-form" target="_blank" novalidate=""><div id="mc_embed_signup_scroll"><input class="email" id="mce-EMAIL" type="email" value="" name="EMAIL" placeholder="email address" required=""><div style="position: absolute; left: -5000px;" aria-hidden="true"><input type="text" name="b_1842e6560d6e10316b4e1aaf5_76586bdcf4" tabindex="-1" value=""></div><div class="clear"><input class="button" id="mc-embedded-subscribe" type="submit" value="Subscribe" name="subscribe"></div></div></form></div><div class="infoHeadline"><h2>Have Questions?</h2></div><p> Ask us questions at our   <a href="https://groups.google.com/forum/#!forum/squad-stanford-qa">google group</a> or at <a href="mailto:pranavsr@stanford.edu">pranavsr@stanford.edu</a> and <a href="mailto:robinjia@stanford.edu">robinjia@stanford.edu</a>.</p></div><div class="infoSubheadline"><a href="https://twitter.com/share" class="twitter-share-button" data-url="https://stanford-qa.com" data-text="The Stanford Question Answering Dataset - 100,000+ questions for reading comprehension" data-via="stanfordnlp" data-size="large" data-hashtags="SQuAD">Tweet</a> <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script><!-- Place this tag where you want the button to render. -->
<a class="github-button" href="https://github.com/rajpurkar/SQuAD-explorer" data-icon="octicon-star" data-style="mega" data-count-href="/rajpurkar/SQuAD-explorer/stargazers" data-count-api="/repos/rajpurkar/SQuAD-explorer#stargazers_count" data-count-aria-label="# stargazers on GitHub" aria-label="Star rajpurkar/SQuAD-explorer on GitHub">Star</a></div></div></div><div class="col-md-7"><div class="infoCard"><div class="infoBody"><div class="infoHeadline"><h2>Leaderboard</h2></div><p>SQuAD2.0 tests the ability of a system to not only answer reading comprehension questions, but also abstain when presented with a question that cannot be answered based on the provided paragraph.  How will your system compare to humans on this task? </p><table class="table performanceTable"><tr><th>Rank</th><th>Model</th><th>EM</th><th>F1</th></tr><tr class="human-row"><td></td><td>Human Performance<p class="institution">Stanford University</p><a href="http://arxiv.org/abs/1606.05250">(Rajpurkar & Jia et al. '18)</a></td><td>86.831</td><td>89.452</td></tr><tr><td> <p>1</p><span class="date label label-default">Mar 20, 2019</span></td><td style="word-break:break-word;">BERT + DAE + AoA (ensemble)<p class="institution">Joint Laboratory of HIT and iFLYTEK Research</p></td><td><b>87.147</b></td><td><b>89.474</b></td></tr><tr><td> <p>2</p><span class="date label label-default">Mar 15, 2019</span></td><td style="word-break:break-word;">BERT + ConvLSTM + MTL + Verifier (ensemble)<p class="institution">Layer 6 AI</p></td><td>86.730</td><td>89.286</td></tr><tr><td> <p>3</p><span class="date label label-default">Mar 05, 2019</span></td><td style="word-break:break-word;">BERT + N-Gram Masking + Synthetic Self-Training (ensemble)<p class="institution">Google AI Language</p><a class="link" href="https://github.com/google-research/bert">https://github.com/google-research/bert</a></td><td>86.673</td><td>89.147</td></tr><tr><td> <p>4</p><span class="date label label-default">Apr 13, 2019</span></td><td style="word-break:break-word;">BERT++(ensemble)<p class="institution">anonymous</p></td><td>86.166</td><td>88.886</td></tr><tr><td> <p>5</p><span class="date label label-default">Mar 16, 2019</span></td><td style="word-break:break-word;">BERT + DAE + AoA (single model)<p class="institution">Joint Laboratory of HIT and iFLYTEK Research</p></td><td>85.884</td><td>88.621</td></tr><tr><td> <p>6</p><span class="date label label-default">Apr 12, 2019</span></td><td style="word-break:break-word;">SemBERT (ensemble model)<p class="institution">Shanghai Jiao Tong University</p></td><td>85.703</td><td>88.400</td></tr><tr><td> <p>7</p><span class="date label label-default">Apr 11, 2019</span></td><td style="word-break:break-word;">SemBERT (single model)<p class="institution">Shanghai Jiao Tong University</p></td><td>84.800</td><td>87.864</td></tr><tr><td> <p>7</p><span class="date label label-default">Apr 16, 2019</span></td><td style="word-break:break-word;">Insight-baseline-BERT (single model)<p class="institution">PAII Insight Team</p></td><td>84.834</td><td>87.644</td></tr><tr><td> <p>8</p><span class="date label label-default">Apr 13, 2019</span></td><td style="word-break:break-word;">BERT++(single model)<p class="institution">anonymous</p></td><td>84.620</td><td>87.625</td></tr><tr><td> <p>8</p><span class="date label label-default">Jan 15, 2019</span></td><td style="word-break:break-word;">BERT + MMFT + ADA (ensemble)<p class="institution">Microsoft Research Asia</p></td><td>85.082</td><td>87.615</td></tr><tr><td> <p>8</p><span class="date label label-default">Mar 13, 2019</span></td><td style="word-break:break-word;">BERT + ConvLSTM + MTL + Verifier (single model)<p class="institution">Layer 6 AI</p></td><td>84.924</td><td>88.204</td></tr><tr><td> <p>8</p><span class="date label label-default">Mar 05, 2019</span></td><td style="word-break:break-word;">BERT + N-Gram Masking + Synthetic Self-Training (single model)<p class="institution">Google AI Language</p><a class="link" href="https://github.com/google-research/bert">https://github.com/google-research/bert</a></td><td>85.150</td><td>87.715</td></tr><tr><td> <p>9</p><span class="date label label-default">Jan 10, 2019</span></td><td style="word-break:break-word;">BERT + Synthetic Self-Training (ensemble)<p class="institution">Google AI Language</p><a class="link" href="https://github.com/google-research/bert">https://github.com/google-research/bert</a></td><td>84.292</td><td>86.967</td></tr><tr><td> <p>10</p><span class="date label label-default">Dec 21, 2018</span></td><td style="word-break:break-word;">PAML+BERT (ensemble model)<p class="institution">PINGAN GammaLab</p></td><td>83.457</td><td>86.122</td></tr><tr><td> <p>10</p><span class="date label label-default">Dec 13, 2018</span></td><td style="word-break:break-word;">BERT finetune baseline (ensemble)<p class="institution">Anonymous</p></td><td>83.536</td><td>86.096</td></tr><tr><td> <p>11</p><span class="date label label-default">Dec 16, 2018</span></td><td style="word-break:break-word;">Lunet + Verifier + BERT (ensemble)<p class="institution">Layer 6 AI NLP Team</p></td><td>83.469</td><td>86.043</td></tr><tr><td> <p>11</p><span class="date label label-default">Mar 20, 2019</span></td><td style="word-break:break-word;">Bert-raw (ensemble)<p class="institution">None</p></td><td>83.604</td><td>86.036</td></tr><tr><td> <p>12</p><span class="date label label-default">Dec 15, 2018</span></td><td style="word-break:break-word;">Lunet + Verifier + BERT (single model)<p class="institution">Layer 6 AI NLP Team</p></td><td>82.995</td><td>86.035</td></tr><tr><td> <p>12</p><span class="date label label-default">Jan 14, 2019</span></td><td style="word-break:break-word;">BERT + MMFT + ADA (single model)<p class="institution">Microsoft Research Asia</p></td><td>83.040</td><td>85.892</td></tr><tr><td> <p>13</p><span class="date label label-default">Jan 10, 2019</span></td><td style="word-break:break-word;">BERT + Synthetic Self-Training (single model)<p class="institution">Google AI Language</p><a class="link" href="https://github.com/google-research/bert">https://github.com/google-research/bert</a></td><td>82.972</td><td>85.810</td></tr><tr><td> <p>13</p><span class="date label label-default">Feb 26, 2019</span></td><td style="word-break:break-word;">BERT with Something (ensemble)<p class="institution">Anonymous</p></td><td>83.051</td><td>85.737</td></tr><tr><td> </table></td></tr></table></div></div></body></html>