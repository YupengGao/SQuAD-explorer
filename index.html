<!DOCTYPE html><!--Author: Pranav Rajpurkar 2016--><html><head><meta charset="utf-8"><title>IBM_competition</title><meta name="description" content="Stanford Question Answering Dataset (SQuAD) is a new reading comprehension dataset, consisting of questions posed by crowdworkers on a set of Wikipedia articles, where the answer to every question is a segment of text, or span, from the corresponding reading passage. With 100,000+ question-answer pairs on 500+ articles, SQuAD is significantly larger than previous reading comprehension datasets."><meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, user-scalable=no"><!-- <meta property="og:image" content="/logo.png"><link rel="image_src" type="image/png" href="/ibm_competition/logo.png"> --><!-- <link rel="shortcut icon" href="/ibm_competition/favicon.ico" type="image/x-icon"><link rel="icon" href="/ibm_competition/favicon.ico" type="image/x-icon"> --><link rel="stylesheet" href="/ibm_competition/bower_components/bootstrap/dist/css/bootstrap.min.css"><link rel="stylesheet" href="/ibm_competition/stylesheets/layout.css"><link rel="stylesheet" href="/ibm_competition/stylesheets/index.css"><script async defer src="https://buttons.github.io/buttons.js"></script><script src="/ibm_competition/javascripts/analytics.js"></script></head><body><div class="navbar navbar-default navbar-fixed-top" id="topNavbar" role="navigation"><div class="container clearfix" id="navContainer"><div class="rightNav"><div class="collapseDiv"><button class="navbar-toggle collapsed" type="button" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar"><span class="glyphicon glyphicon-menu-hamburger"></span></button></div><div class="collapse navbar-collapse" id="navbar"><ul class="nav navbar-nav navbar-right"><li><a href="/ibm_competition/">Home</a></li></ul></div></div><div class="leftNav"><div class="brandDiv"><a class="navbar-brand" href="/ibm_competition/">IBM</a></div></div></div></div><div class="cover" id="topCover"><div class="container"><div class="row"><div class="col-md-12"><h1 id="appTitle">Fashion Image Retrieval Challenge</h1></div></div></div></div><div class="cover" id="contentCover"><div class="container"><div class="row"><div class="col-md-5"><div class="infoCard"><div class="infoBody"><div class="infoHeadline"><h2>What is the tast?</h2></div><p> The challenge is an image retrieval task, where the input query is specified in the form of an image with its contextual information and some natural language text that describes visual difference to the input image. For example, we may present an image of a white dress, and ask the system to find images which are in the same style and color, but are modified in small ways, such as adding a belt in the waist.  To solve this task, the system needs to learn a similarity metric between the target image and a reference image plus text such that the similarity score is high if and only if the target image is a positive match to the input query (the reference image and the text). The retrieval interface setup with natural language user input offers a natural and effective interface for image retrieval [1] and serves as an important step towards understanding and modeling full-blown conversational image search retrieval [2].</p><hr><p><i>[1] Composing Text and Image for Image Retrieval - An Empirical Odyssey. Nam Vo, Lu Jiang, Chen Sun, Kevin Murphy, Li-Jia Li, Li Fei-Fei, James Hays. CVPR 2019. <br>[2] Dialog-based interactive image retrieval. Xiaoxiao Guo*, Hui Wu*, Yu Cheng, Steve Rennie, Gerald Tesauro and Rogerio Feris. NeurIPS 2018.</i></p>  <div class="infoHeadline"><h2>Dataset</h2></div><p>The dataset consists of pairs of fashion images (a search target and a reference image) and human written sentences describing the visual differences between the target image and the reference image. The dataset consists of four subsets, each featuring a specific fashion category (shoes, men's shirts, women's tops and dresses). Given this dataset, we will run a competition of retrieving images conditioned on a reference image and the associated natural language annotation. The quality of the retrieval results will be evaluated using a held-out test dataset.</p><hr><div class="infoHeadline"><h2>Getting Started</h2></div><p>We've built a few resources to help you get started with the dataset.</p><p> Download a copy of the dataset (distributed under the  <a href="http://creativecommons.org/licenses/by-sa/4.0/legalcode">CC BY-SA 4.0</a> license):<ul class="list-unstyled"><li><a class="btn actionBtn inverseBtn" href="/ibm_competition/dataset/train-v2.0.json" download>Training Set v2.0 (40 MB)</a></li><li><a class="btn actionBtn inverseBtn" href="/ibm_competition/dataset/dev-v2.0.json" download>Dev Set v2.0 (4 MB)</a></li></ul></p><p>To evaluate your models, we have also made available the evaluation script we will use for official evaluation, along with a sample prediction file that the script will take as input. To run the evaluation, use <code>python evaluate-v2.0.py &lt;path_to_dev-v2.0&gt; &lt;path_to_predictions&gt;</code>.<ul class="list-unstyled"><li><a class="btn actionBtn inverseBtn" href="https://worksheets.codalab.org/rest/bundles/0x6b567e1cf2e041ec80d7098f031c5c9e/contents/blob/" download>Evaluation Script v2.0</a></li><li><a class="btn actionBtn inverseBtn" href="https://worksheets.codalab.org/bundles/0x8731effab84f41b7b874a070e40f61e2/" download>Sample Prediction File (on Dev v2.0)</a></li></ul></p><p>Once you have a built a model that works to your expectations on the dev set, you submit it to get official scores on the dev and a hidden test set. To preserve the integrity of test results, we do not release the test set to the public. Instead, we require you to submit your model so that we can run it on the test set for you. Here's a tutorial walking you through official evaluation of your model:</p><a class="btn actionBtn inverseBtn" href="https://worksheets.codalab.org/worksheets/0x8212d84ca41c4150b555a075b19ccc05/">Submission Tutorial</a><div class="infoHeadline"><h2>Have Questions?</h2></div><p> Ask us questions at our   <a href="https://groups.google.com/forum/#!forum/squad-stanford-qa">google group</a> or at <a href="mailto:pranavsr@stanford.edu">pranavsr@stanford.edu</a> and <a href="mailto:robinjia@stanford.edu">robinjia@stanford.edu</a>.</p></div><div class="infoSubheadline"><!-- <a href="https://twitter.com/share" class="twitter-share-button" data-url="https://stanford-qa.com" data-text="The Stanford Question Answering Dataset - 100,000+ questions for reading comprehension" data-via="stanfordnlp" data-size="large" data-hashtags="SQuAD">Tweet</a> --> <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script><!-- Place this tag where you want the button to render.-->
<!-- <a class="github-button" href="https://github.com/yupenggao/ibm_competition" data-icon="octicon-star" data-style="mega" data-count-href="/yupenggao/ibm_competition/stargazers" data-count-api="/repos/yupenggao/ibm_competition#stargazers_count" data-count-aria-label="# stargazers on GitHub" aria-label="Star rajpurkar/ibm_competition on GitHub">Star</a> --></div></div></div><div class="col-md-7"><div class="infoCard"><div class="infoBody"><div class="infoHeadline"><h2>Leaderboard</h2></div><p>SQuAD2.0 tests the ability of a system to not only answer reading comprehension questions, but also abstain when presented with a question that cannot be answered based on the provided paragraph.  How will your system compare to humans on this task? </p>

<table class="table performanceTable">
	<tr>
		<th>Rank</th><th>Model</th><th>EM</th><th>F1</th>
	</tr>
	<tr class="human-row"><td></td><td>Human Performance<p class="institution">Stanford University</p><a href="http://arxiv.org/abs/1606.05250">(Rajpurkar & Jia et al. '18)</a></td><td>86.831</td><td>89.452</td>
	</tr>

	<tr>
		<td> 
			<p>1</p>
			<span class="date label label-default">Mar 20, 2019</span>
		</td>
		<td style="word-break:break-word;">BERT + DAE + AoA (ensemble)
			<p class="institution">Joint Laboratory of HIT and iFLYTEK Research</p>
		</td>
		<td>
			<b>87.147</b>
		</td>
		<td>
			<b>89.474</b>
		</td>
	</tr>

	<tr>
		<td> 
			<p>2</p>
			<span class="date label label-default">Mar 15, 2019</span>
		</td>
		<td style="word-break:break-word;">BERT + ConvLSTM + MTL + Verifier (ensemble)
			<p class="institution">Layer 6 AI</p>
		</td>
		<td>86.730</td>
		<td>89.286</td>
	</tr>

	<tr>
		<td> 
			<p>3</p>
			<span class="date label label-default">Mar 05, 2019</span>
		</td>
		<td style="word-break:break-word;">BERT + N-Gram Masking + Synthetic Self-Training (ensemble)
			<p class="institution">Google AI Language</p>
			<a class="link" href="https://github.com/google-research/bert">https://github.com/google-research/bert</a>
		</td>
		<td>86.673</td>
		<td>89.147</td>
	</tr>
</table></div></div>

<div class="infoCard"><div class="infoBody"><div class="infoHeadline"><h2>SQuAD1.1 Leaderboard </h2></div><p>  Here are the ExactMatch (EM) and F1 scores evaluated on the test set of SQuAD v1.1.</p>
<table class="table performanceTable">
	<tr>
		<th>Rank</th><th>Model</th><th>EM</th><th>F1</th>
	</tr>
	<tr class="human-row"><td></td>
		<td>Human Performance
		<p class="institution">Stanford University</p>
		<a href="http://arxiv.org/abs/1606.05250">(Rajpurkar et al. '16)</a>
		</td>
		<td>82.304</td><td>91.221</td>
	</tr>
	<tr>
		<td> 
			<p>1</p>
			<span class="date label label-default">Oct 05, 2018</span>
		</td>
		<td style="word-break:break-word;">BERT (ensemble)
			<p class="institution">Google AI Language</p>
			<a class="link" href="https://arxiv.org/abs/1810.04805">https://arxiv.org/abs/1810.04805</a>
		</td>
		<td>
		<b>87.433</b></td><td><b>93.160</b>
		</td>
	</tr>
</table>
</div></div></div></div></div></body></html>
